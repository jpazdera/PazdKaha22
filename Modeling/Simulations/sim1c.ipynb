{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation 1c: Fitting 6-Item Lists\n",
    "\n",
    "This notebook interacts with part of Simulation 3 from the paper - fitting the pooled-modality data from 6-item lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import CMR2_pack_cyth as CMR2\n",
    "from glob import glob\n",
    "from cluster_helper.cluster import cluster_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter(x, y, s=20, c='b', marker='o', cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Creates a jittered scatterplot, for use in plotting particle swarms.\n",
    "    \"\"\"\n",
    "    stdev = .12\n",
    "    jittered = x + np.random.randn(len(x)) * stdev\n",
    "    return plt.scatter(jittered, y, s=s, c=c, marker=marker, cmap=cmap, norm=norm, vmin=vmin, vmax=vmax, alpha=alpha, linewidths=linewidths, verts=verts, **kwargs)\n",
    "\n",
    "def read_err_files(paths):\n",
    "    \"\"\"\n",
    "    Read in the error values from each file in the path list.\n",
    "    \"\"\"\n",
    "    # Return an empty array if no paths are provided\n",
    "    if len(paths) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Load first file to determine number of particles and parameters\n",
    "    scores = np.loadtxt(paths[0], delimiter=' ')\n",
    "    n_iter = len(paths)\n",
    "    n_particles = scores.shape[0]\n",
    "    \n",
    "    # Create a matrix of iterations x particles\n",
    "    err_vals = np.empty((n_iter, n_particles))\n",
    "    err_vals.fill(np.nan)\n",
    "    for i, path in enumerate(paths):\n",
    "        scores = np.loadtxt(path, delimiter=' ')\n",
    "        if len(scores.shape) == 2:\n",
    "            err_vals[i, :len(scores)] = scores[:, 0]\n",
    "        else:\n",
    "            err_vals[i, :len(scores)] = scores\n",
    "\n",
    "    return err_vals\n",
    "\n",
    "def read_xfiles(paths):\n",
    "    \"\"\"\n",
    "    Read in the particle locations from each file in the path list.\n",
    "    \"\"\"\n",
    "    # Return an empty array if no paths are provided\n",
    "    if len(paths) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Load first xfile to determine number of particles and parameters\n",
    "    params = np.loadtxt(paths[0], delimiter=' ')\n",
    "    n_iter = len(paths)\n",
    "    n_particles, n_params = params.shape\n",
    "    \n",
    "    # Create a matrix of iterations x particles x parameters\n",
    "    param_values = np.empty((n_iter, n_particles, n_params))\n",
    "    param_values.fill(np.nan)\n",
    "    for i, path in enumerate(paths):\n",
    "        params = np.loadtxt(path, delimiter=' ')\n",
    "        param_values[i, :len(params), :] = params\n",
    "        \n",
    "    return param_values\n",
    "\n",
    "def load_stats(n, i, p, done=True):\n",
    "    \"\"\"\n",
    "    Load the behavioral stats pickle file generated by run n, iteration i, particle p.\n",
    "    \"\"\"\n",
    "    path = '/scratch/jpazdera/cmr2/param_saves/out%s/%sdata%s.pkl' % (n, i, p) if done else '/scratch/jpazdera/cmr2/outfiles/%sdata%s.pkl' % (i, p)\n",
    "    with open(path, 'rb') as f:\n",
    "        cmr_stats = pkl.load(f)\n",
    "    return cmr_stats\n",
    "\n",
    "def load_targets(target_stat_file):\n",
    "    # Load target stats from JSON file\n",
    "    with open(target_stat_file, 'r') as f:\n",
    "        targets = json.load(f)\n",
    "    for key in targets:\n",
    "        if isinstance(targets[key], list):\n",
    "            targets[key] = np.array(targets[key], dtype=float)\n",
    "        if isinstance(targets[key], dict):\n",
    "            for subkey in targets[key]:\n",
    "                if isinstance(targets[key][subkey], list):\n",
    "                    targets[key][subkey] = np.array(targets[key][subkey], dtype=float)\n",
    "    return targets\n",
    "\n",
    "def plot_fit(data, model, savefile=None):\n",
    "    \"\"\"\n",
    "    Plot model performance against target data.\n",
    "    \"\"\"\n",
    "    VIS_FMT = 'k-'\n",
    "    AUD_FMT = 'k--'\n",
    "    ERR_VIS = 'k'\n",
    "    ERR_AUD = 'k'\n",
    "    ERR_ALPHA = .2\n",
    "    ERR_ALPHA2 = .075\n",
    "\n",
    "    SMALL_SIZE = 12\n",
    "    MEDIUM_SIZE = 14\n",
    "    LARGE_SIZE = 16\n",
    "\n",
    "    plt.rc('font', size=LARGE_SIZE)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=LARGE_SIZE)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=LARGE_SIZE)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=LARGE_SIZE)  # fontsize of the figure title\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    ax=plt.subplot(221)\n",
    "    plt.title('SPC')\n",
    "    plt.plot(range(1, 7), np.array(data['spc']['6']), VIS_FMT, alpha=.3)\n",
    "    plt.plot(range(1, 7), np.array(model['spc']['6']), AUD_FMT)\n",
    "    plt.fill_between(range(1, 7), np.add(model['spc']['6'], model['spc_sem']['6']), np.subtract(model['spc']['6'], model['spc_sem']['6']), alpha=ERR_ALPHA, color=ERR_AUD)\n",
    "    plt.xticks(range(1, 7))\n",
    "    plt.xlabel('Serial Position')\n",
    "    plt.ylabel('Recall Prob.')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(['Data', 'Model'], loc=3)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    ax=plt.subplot(222)\n",
    "    plt.title('PFR')\n",
    "    plt.plot(range(1, 7), np.array(data['pfr']['6']), VIS_FMT, alpha=.3)\n",
    "    plt.plot(range(1, 7), np.array(model['pfr']['6']), AUD_FMT)\n",
    "    plt.fill_between(range(1, 7), np.add(model['pfr']['6'], model['pfr_sem']['6']), np.subtract(model['pfr']['6'], model['pfr_sem']['6']), alpha=ERR_ALPHA, color=ERR_AUD)\n",
    "    plt.xticks(range(1, 7))\n",
    "    plt.xlabel('Serial Position')\n",
    "    plt.ylabel('Prob. of First Recall')\n",
    "    plt.ylim(0, .75)\n",
    "    plt.yticks([0, .15, .3, .45, .6, .75])\n",
    "    plt.legend(['Data', 'Model'], loc=1)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    ax=plt.subplot(223)\n",
    "    plt.title('SPC (Start=SP1)')\n",
    "    plt.plot(range(1, 7), np.array(data['spc_fr1']['6']), VIS_FMT, alpha=.3)\n",
    "    plt.plot(range(1, 7), np.array(model['spc_fr1']['6']), AUD_FMT)\n",
    "    plt.fill_between(range(1, 7), np.add(model['spc_fr1']['6'], model['spc_fr1_sem']['6']), np.subtract(model['spc_fr1']['6'], model['spc_fr1_sem']['6']), alpha=ERR_ALPHA, color=ERR_AUD)\n",
    "    plt.xticks(range(1, 7))\n",
    "    plt.xlabel('Serial Position')\n",
    "    plt.ylabel('Recall Prob.')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(['Data', 'Model'], loc=3)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    ax=plt.subplot(224)\n",
    "    plt.title('SPC (Start=L4)')\n",
    "    plt.plot(range(1, 7), np.array(data['spc_frl4']['6']), VIS_FMT, alpha=.3)\n",
    "    plt.plot(range(1, 7), np.array(model['spc_frl4']['6']), AUD_FMT)\n",
    "    plt.fill_between(range(1, 7), np.add(model['spc_frl4']['6'], model['spc_frl4_sem']['6']), np.subtract(model['spc_frl4']['6'], model['spc_frl4_sem']['6']), alpha=ERR_ALPHA, color=ERR_AUD)\n",
    "    plt.xticks(range(1, 7))\n",
    "    plt.xlabel('Serial Position')\n",
    "    plt.ylabel('Recall Prob.')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(['Data', 'Model'], loc=3)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if savefile is not None:\n",
    "        plt.gcf().savefig(savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sessions with 6-item lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "n_sess = 2500\n",
    "n_trials = 16\n",
    "list_length = 6\n",
    "\n",
    "# Load ltpFR3 wordpool\n",
    "wp = np.loadtxt('word_IDs.txt', dtype='U256')\n",
    "if len(wp) < n_trials * list_length:\n",
    "    raise ValueError('Wordpool is not large enough to support that long of a session!')\n",
    "\n",
    "# Create N sessions with 16 trials of 6 words\n",
    "presw = np.zeros((n_sess, n_trials, list_length), dtype=int)\n",
    "for i in range(n_sess):\n",
    "    presw[i, :, :] = np.random.choice(wp, size=(n_trials, list_length), replace=False)\n",
    "d = {'pres_words': presw.tolist()}\n",
    "\n",
    "# Save word lists to a json file\n",
    "with open('sim1c_lists.json', 'w') as f:\n",
    "    json.dump(d, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create target stat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximated SPCs from GrenEtal17 using https://apps.automeris.io/wpd/\n",
    "ge17_spc_a = np.array([.832, .585, .457, .566, .705, .881])\n",
    "ge17_spc_v = np.array([.754, .585, .538, .554, .566, .566])\n",
    "ge17_fr1_a = np.array([1., .648, .431, .532, .633, .911])\n",
    "ge17_fr1_v = np.array([1., .693, .532, .592, .434, .412])\n",
    "ge17_frl4_a = np.array([.650, .440, .481, .646, .792, .922])\n",
    "ge17_frl4_v = np.array([.541, .425, .496, .545, .717, .743])\n",
    "\n",
    "# PFR data obtained directly from GrenEtal17 Appendix C1\n",
    "ge17_pfr_a = np.array([.61, .03, .03, .06, .08, .12]) \n",
    "ge17_pfr_v = np.array([.50, .03, .09, .09, .13, .13])\n",
    "\n",
    "# Target stats are the average of the visual and auditory data\n",
    "targets = {}\n",
    "targets['spc'] = {}\n",
    "targets['spc']['6'] = ((ge17_spc_a + ge17_spc_v) / 2).tolist()\n",
    "targets['spc_fr1'] = {}\n",
    "targets['spc_fr1']['6'] = ((ge17_fr1_a + ge17_fr1_v) / 2).tolist()\n",
    "targets['spc_frl4'] = {}\n",
    "targets['spc_frl4']['6'] = ((ge17_frl4_a + ge17_frl4_v) / 2).tolist()\n",
    "targets['pfr'] = {}\n",
    "targets['pfr']['6'] = ((ge17_pfr_a + ge17_pfr_v) / 2).tolist()\n",
    "\n",
    "# Save stats to a JSON file for access by the particle swarm\n",
    "with open('../CMR2/target_stats_sim1c.json', 'w') as f:\n",
    "    json.dump(targets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive Completed Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_num = 302\n",
    "\n",
    "if os.path.exists('/scratch/jpazdera/cmr2/param_saves/out%s' % fit_num):\n",
    "    print('Save files already exist! Check to make sure the fit number is correct...')\n",
    "else:\n",
    "    # Copy optimal parameters to opt.txt file\n",
    "    os.system('cp /scratch/jpazdera/cmr2/outfiles/xopt_ltpFR3.txt /scratch/jpazdera/cmr2/param_saves/opt%s.txt' % fit_num)\n",
    "    # Clean up noise files\n",
    "    os.system('rm /scratch/jpazdera/cmr2/noise_files/*')\n",
    "    # Archive outfiles folder\n",
    "    os.system('mv /scratch/jpazdera/cmr2/outfiles/ /scratch/jpazdera/cmr2/param_saves/out%s' % fit_num)\n",
    "    # Create a new outfiles folder\n",
    "    os.system('mkdir /scratch/jpazdera/cmr2/outfiles/')\n",
    "    # Remove tempfiles from archive\n",
    "    os.system('rm /scratch/jpazdera/cmr2/param_saves/out%s/*temp*' % fit_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Clean outfiles without archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_num = 999\n",
    "# Copy optimal parameters to opt.txt file\n",
    "os.system('cp /scratch/jpazdera/cmr2/outfiles/xopt_ltpFR3.txt /scratch/jpazdera/cmr2/param_saves/opt%s.txt' % fit_num)\n",
    "# Clean up noise files\n",
    "os.system('rm /scratch/jpazdera/cmr2/noise_files/*')\n",
    "# Archive outfiles folder\n",
    "os.system('mv /scratch/jpazdera/cmr2/outfiles/ /scratch/jpazdera/cmr2/param_saves/out%s' % fit_num)\n",
    "# Create a new outfiles folder\n",
    "os.system('mkdir /scratch/jpazdera/cmr2/outfiles/')\n",
    "# Remove all files\n",
    "os.system('rm -r /scratch/jpazdera/cmr2/param_saves/out%s' % fit_num)\n",
    "os.system('rm -r /scratch/jpazdera/cmr2/param_saves/opt%s.txt' % fit_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarm Plotting\n",
    "The scripts that runs the particle swarms can be found in ../CMR2/fitting/pso_cmr2_ltpFR3.py and ../CMR2/fitting/genalg_cmr2_ltpFR3.py. Particles/individuals can be processed in parallel by running multiple instances of the script.\n",
    "### Plotting parameter sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#\n",
    "#   SETTINGS\n",
    "#\n",
    "############\n",
    "\n",
    "fit_num = 302\n",
    "done = True\n",
    "folder_path = '/scratch/jpazdera/cmr2/param_saves/out%s/' % fit_num if done else '/scratch/jpazdera/cmr2/outfiles/'\n",
    "\n",
    "#    [ b_e,  b_r, g_fc, g_cf, p_s, p_d,  k,  e, s_cf, b_rp,  o,  a, c_t,   l]\n",
    "lb = [  .1,    0,    0,  .15,   0,   0,  0,  0,   .5,    0,  5, .5,   0,   0]\n",
    "ub = [  .9,    1,    1,  .85,   8,   5, .5, .5,    3,    1, 20,  1,  .5, .25]\n",
    "\n",
    "# Set labels for each parameter's graph\n",
    "param_labels = [\n",
    "    r'$\\beta_{enc}$',\n",
    "    r'$\\beta_{rec}$',\n",
    "    r'$\\gamma_{FC}$',\n",
    "    r'$\\gamma_{CF}$',\n",
    "    r'$\\phi_{s}$',\n",
    "    r'$\\phi_{d}$',\n",
    "    r'$\\kappa$',\n",
    "    r'$\\eta$',\n",
    "    r'$s$',\n",
    "    r'$\\beta_{post}$',\n",
    "    r'$\\omega$',\n",
    "    r'$\\alpha$',\n",
    "    r'$c_{thresh}$',\n",
    "    r'$\\lambda$',\n",
    "    r'$\\beta_{source}$',\n",
    "    r'$\\gamma_{source}$'\n",
    "]\n",
    "\n",
    "############\n",
    "#\n",
    "#   Data preparation\n",
    "#\n",
    "############\n",
    "\n",
    "# Get all parameter and error value file paths\n",
    "xfile_paths = glob(folder_path + '*xfile*')\n",
    "err_paths = glob(folder_path + 'err_iter*')\n",
    "\n",
    "# Sort paths numerically, rather than alphabetically\n",
    "xfile_paths.sort(key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
    "err_paths.sort(key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
    "\n",
    "# Load parameter and error values\n",
    "param_vals = read_xfiles(xfile_paths)  # iteration x particle x param\n",
    "err_vals = np.sqrt(read_err_files(err_paths))  # iteration x particle\n",
    "\n",
    "# Determine number of iterations, swarm size, and number of parameters from shape of param matrix\n",
    "n_iter, swarmsize, n_params = param_vals.shape\n",
    "\n",
    "# Get range of iteration values for plotting purposes\n",
    "iter_vals = range(n_iter)\n",
    "\n",
    "############\n",
    "#\n",
    "#   Plotting\n",
    "#\n",
    "############\n",
    "\n",
    "plt.figure(figsize=(10, 28))\n",
    "# Plot one row in the figure for each parameter\n",
    "for param_num in range(n_params):\n",
    "\n",
    "    # Select the subplot for the current parameter, and label the axes\n",
    "    plt.subplot(n_params, 1, param_num+1)\n",
    "    plt.ylabel(param_labels[param_num])\n",
    "    plt.yticks([lb[param_num], lb[param_num] + (ub[param_num] - lb[param_num]) / 2, ub[param_num]])\n",
    "    \n",
    "    positions = np.array([])\n",
    "    iterations = np.array([])\n",
    "    err_sorted = np.array([])\n",
    "    for i in iter_vals:\n",
    "        # Sort positions and error vals from worst to best fit, so that best fits always appear in front\n",
    "        sort_idx = np.argsort(1/err_vals[i, :])\n",
    "        positions = np.concatenate((positions, param_vals[i, sort_idx, param_num]))\n",
    "        err_sorted = np.concatenate((err_sorted, err_vals[i, sort_idx]))\n",
    "        iterations = np.concatenate((iterations, [i for _ in range(swarmsize)]))\n",
    "    \n",
    "    jitter(iterations+1, positions, s=2, alpha=1, c=1/err_sorted, cmap=cm.rainbow)#, vmin=1/1000, vmax = 1/100)\n",
    "    plt.ylim(lb[param_num], ub[param_num])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.gcf().savefig('CMR2_plots/param_plot_%s.jpg' % fit_num, dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting error values over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_num = 302\n",
    "done = True\n",
    "saved_files_path = '/scratch/jpazdera/cmr2/param_saves/out%s/' % fit_num if done else '/scratch/jpazdera/cmr2/outfiles/'\n",
    "\n",
    "# Get all error file paths\n",
    "err_paths = glob(saved_files_path+'err_iter*')\n",
    "\n",
    "# Sort paths numerically, rather than alphabetically\n",
    "err_paths.sort(key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
    "\n",
    "# Read error values\n",
    "err_array = np.sqrt(read_err_files(err_paths))\n",
    "\n",
    "# Determine number of iterations\n",
    "nits = err_array.shape[0]\n",
    "\n",
    "# Determine minimum and mean error in each iteration\n",
    "min_errs = np.nanmin(err_array, axis=1)\n",
    "mean_errs = np.nanmean(err_array, axis=1)\n",
    "print('\\nIterations completed: ', len(min_errs))\n",
    "print(\"\\nSmallest error is at iteration:\")\n",
    "smallest_err_loc = np.nanargmin(min_errs) + 1  # Add one since iteration numbers start indexing at 1\n",
    "print(smallest_err_loc)\n",
    "print(\"And its value is: \")\n",
    "print(np.nanmin(min_errs))\n",
    "\n",
    "# Plot error over time\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(121)\n",
    "plt.axvline(smallest_err_loc, color='k', linestyle='--')\n",
    "plt.plot(range(1, nits+1), min_errs, 'b-', label=\"Particle Swarm\")\n",
    "plt.title(\"Minimum Error Over Time\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Min Error\")\n",
    "plt.subplot(122)\n",
    "plt.plot(range(1, nits+1), mean_errs, 'b-')\n",
    "plt.title(\"Mean Error Over Time\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Error\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.tight_layout(w_pad=3)\n",
    "plt.gcf().savefig('CMR2_plots/error%s.jpg' % fit_num, dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting best fit from an iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#\n",
    "#   Settings\n",
    "#\n",
    "############\n",
    "\n",
    "fit_num = 302\n",
    "iter_num = 168\n",
    "done = True\n",
    "\n",
    "############\n",
    "#\n",
    "#   Identify Best Fit\n",
    "#\n",
    "############\n",
    "\n",
    "iter_num = str(iter_num)\n",
    "target_stat_file = '../CMR2/target_stats_sim1c.json'\n",
    "err_dir = '/scratch/jpazdera/cmr2/param_saves/out%s/' % fit_num if done else '/scratch/jpazdera/cmr2/outfiles/'\n",
    "err_path = 'err_iter' + iter_num\n",
    "\n",
    "# Load error values\n",
    "err_vals = np.sqrt(np.loadtxt(err_dir + err_path, delimiter=' '))\n",
    "err_indices = [i for i, _ in enumerate(err_vals)]\n",
    "\n",
    "# get minimum err value\n",
    "min_err = np.nanmin(err_vals)\n",
    "\n",
    "# get index where min err value was located\n",
    "min_err_index = int(np.where(err_vals == min_err)[0])\n",
    "\n",
    "print(\"\\nMinimum error and matching index are: \")\n",
    "print(\"Error: \", min_err)\n",
    "print(\"Index: \", min_err_index)\n",
    "\n",
    "# find the set of parameters in the associated xfile that match\n",
    "xfile_path = iter_num + 'xfile.txt'\n",
    "xfile = np.loadtxt(err_dir + xfile_path, delimiter=' ')\n",
    "\n",
    "print(\"\\nBest-fitting parameters for iteration \" + iter_num + \" are: \")\n",
    "[print('%s,' % x) for x in xfile[min_err_index]]\n",
    "\n",
    "############\n",
    "#\n",
    "#   Load Stats\n",
    "#\n",
    "############\n",
    "\n",
    "cmr_stat_file = '/scratch/jpazdera/cmr2/param_saves/out%s/%sdata%s.pkl' % (fit_num, iter_num, min_err_index) if done else '/scratch/jpazdera/cmr2/outfiles/%sdata%s.pkl' % (iter_num, min_err_index)\n",
    "with open(cmr_stat_file, 'rb') as f:\n",
    "    cmr_stats = pkl.load(f)\n",
    "\n",
    "# Load target stats from JSON file\n",
    "targets = load_targets(target_stat_file)\n",
    "\n",
    "############\n",
    "#\n",
    "#   Plot Fit\n",
    "#\n",
    "############\n",
    "\n",
    "plot_fit(targets, cmr_stats)\n",
    "plt.gcf().savefig('CMR2_plots/fit%s.jpg' % fit_num, dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalizing Best Fit\n",
    "\n",
    "The code below identifies the best parameter set found by each particle and uses each to simulate all sessions five times (via eval_model()). Note that the use of cluster helper requires import statements to occur inside the function to be run in parallel, which can be seen in eval_model() below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(param_vec):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pickle as pkl\n",
    "    import CMR2_pack_cyth as CMR2\n",
    "    from glob import glob\n",
    "    \n",
    "    sys.path.append('../CMR2/fitting/')\n",
    "    import optimization_utils as opt\n",
    "    \n",
    "    ##########\n",
    "    #\n",
    "    # Initialization\n",
    "    #\n",
    "    ##########\n",
    "    \n",
    "    # Settings\n",
    "    n_sess = 2500  # Number of sessions to simulate\n",
    "    fit_number = 302\n",
    "    \n",
    "    wordpool_file = '../CMR2/wasnorm_wordpool.txt'  # Path to wordpool file\n",
    "    w2v_file = '../CMR2/w2v.txt'  # Path to semantic associative matrix file\n",
    "    target_stat_file = '../CMR2/target_stats_sim1c.json'  # Path to file with target stats\n",
    "    \n",
    "    # Load randomly generated 6-item lists\n",
    "    with open('sim1c_lists.json', 'r') as f:\n",
    "        data_pres = np.array(json.load(f)['pres_words'][:n_sess])\n",
    "\n",
    "    # Create session indices and collapse sessions and trials of presented items into one dimension\n",
    "    sessions = []\n",
    "    for n, sess_pres in enumerate(data_pres):\n",
    "        sessions += [n for _ in sess_pres]\n",
    "    sessions = np.array(sessions)\n",
    "    data_pres = data_pres.reshape((data_pres.shape[0] * data_pres.shape[1], data_pres.shape[2]))\n",
    "    sources = None\n",
    "\n",
    "    # Load semantic similarity matrix (word2vec)\n",
    "    w2v = np.loadtxt(w2v_file)\n",
    "\n",
    "    # Load target stats from JSON file\n",
    "    with open(target_stat_file, 'r') as f:\n",
    "        targets = json.load(f)\n",
    "    for key in targets:\n",
    "        if isinstance(targets[key], list):\n",
    "            targets[key] = np.array(targets[key], dtype=float)\n",
    "        if isinstance(targets[key], dict):\n",
    "            for subkey in targets[key]:\n",
    "                if isinstance(targets[key][subkey], list):\n",
    "                    targets[key][subkey] = np.array(targets[key][subkey], dtype=float)\n",
    "    \n",
    "    # Extract parallel ID number if one was provided in the parameter vector\n",
    "    if len(param_vec) == 15:\n",
    "        parallel_ID = int(param_vec[-1])\n",
    "        param_vec = param_vec[:-1]\n",
    "    else:\n",
    "        parallel_ID = None\n",
    "    \n",
    "    ##########\n",
    "    #\n",
    "    # Run Model\n",
    "    #\n",
    "    ##########\n",
    "    \n",
    "    # Run model with the parameters given in param_vec\n",
    "    data_path = '/scratch/jpazdera/cmr2/param_saves/out%i/final_data%s.pkl' % (fit_number, parallel_ID)\n",
    "    if os.path.exists(data_path):\n",
    "        with open(data_path, 'rb') as f:\n",
    "            cmr_stats = pkl.load(data_path)\n",
    "    else:\n",
    "        err, cmr_stats = opt.obj_func(param_vec, targets, data_pres, sessions, w2v, sources, return_recalls=False, is_sim1c=True)\n",
    "        with open(data_path, 'wb') as f:\n",
    "            pkl.dump(cmr_stats, f)\n",
    "\n",
    "    return cmr_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 302\n",
    "\n",
    "folder_path = '/scratch/jpazdera/cmr2/param_saves/out%s/' % run\n",
    "target_stat_file = '../CMR2/target_stats_sim1c.json'\n",
    "\n",
    "# Get parameter and error value file paths\n",
    "xfile_paths = glob(folder_path + '*xfile*')\n",
    "err_paths = glob(folder_path + 'err_iter*')\n",
    "\n",
    "# Sort paths numerically, rather than alphabetically\n",
    "xfile_paths.sort(key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
    "err_paths.sort(key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
    "\n",
    "# Load parameter and error values\n",
    "params = read_xfiles(xfile_paths)  # iteration x particle x parameter\n",
    "scores = read_err_files(err_paths)  # iteration x particle\n",
    "\n",
    "# Identify each particle's 5 best parameter sets\n",
    "swarmsize = scores.shape[1]\n",
    "nparams = params.shape[2]\n",
    "best_params = np.full((5 * swarmsize, nparams), np.nan)\n",
    "for i in range(swarmsize):\n",
    "    best_iters = np.argsort(scores[:, i])[:5]\n",
    "    best_params[i*5:i*5+5, :] = params[best_iters, i]\n",
    "\n",
    "# Attach ID numbers to parameter sets\n",
    "best_params = np.hstack((best_params, np.atleast_2d(np.arange(1000)).T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best parameter set found by each particle\n",
    "done = True\n",
    "\n",
    "results = None\n",
    "if done:\n",
    "    with open(folder_path + 'final_results.pkl', 'rb') as f:\n",
    "        results = pkl.load(f)\n",
    "else:\n",
    "    try:\n",
    "        with cluster_view(scheduler='sge', queue='RAM.q', num_jobs=250, cores_per_job=1) as view:\n",
    "            results = view.map(eval_model, best_params)\n",
    "    except IOError as e:\n",
    "        print(e)\n",
    "    with open(folder_path + 'final_results.pkl', 'wb') as f:\n",
    "        pkl.dump(results, f, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify best fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best parameter set and print it along with its error and the number of the particle that found it\n",
    "scores = np.array([r['err'] for r in results])\n",
    "best_err = scores.min()\n",
    "best_idx = scores.argmin()\n",
    "print(best_err, best_idx)\n",
    "print()\n",
    "[print(round(p, 5)) for p in best_params[best_idx]]\n",
    "\n",
    "# Plot behavioral stats for the best fitting parameter\n",
    "targets = load_targets(target_stat_file)\n",
    "                \n",
    "plot_fit(targets, results[best_idx], savefile='/home1/jpazdera/jupyter/ltpFR3/notebooks-analysis/ltpFR3_Figs/sim1c.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
